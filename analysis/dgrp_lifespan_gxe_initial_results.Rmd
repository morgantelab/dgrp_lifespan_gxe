---
title: "Prediction of life span with different models"
author: "Fabio Morgante"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  workflowr::wflow_html:
    toc: true
editor_options:
  chunk_output_type: console
---

```{r set options, message=FALSE}
###Load libraries
library(ggplot2)
library(cowplot)
library(dplyr)

prefix <- "dgrp_lifespan_gxe"
n_reps <- 20
```

## Introduction

The goal of this analysis is to assess the performance of the linear mixed models including genotype only, environment only,
genotype + environment, genotype + environment + genotype $\times$ environment information at predicting life span in the 
DGRP.

We used data from [Huang et al. (2020)](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000645), which
has life span measurements for $n=176$ lines at 3 temperatures (18C, 25C, 28C) for the 2 sexes (coded as 1=females and 0=males), 
which results in $q=1,056$ observations. 
The authors showed that there is extensive $G \times E$ affecting life span. In the present work, we used both temperature 
and sex as $c=2$ environmental variables, for a total of $r=6$ environments.

We fitted the following linear mixed models:

* G-BLUP -- $y = \mu + g + \epsilon$.
* E-BLUP -- $y = \mu + e + \epsilon$.
* GE-BLUP -- $y = \mu + g + e + \epsilon$.
* GxE-BLUP -- $y = \mu + g + e + ge + \epsilon$.

where

$y$ is a $q$-vector of phenotypic observations, 
$\mu$ is a $q$-vector of intercept values, 
$g$ is a $q$-vector of additive genetic values, $g \sim N_q(\mathbf O, \mathbf{ZGZ^\intercal} \sigma^2_g)$, 
$\mathbf Z$ is a $q \times n$ incidence matrix, 
$\mathbf G$ is a $n \times n$ genomic relationship matrix (GRM), 
$e$ is a $q$-vector of environmental values, $e \sim N_q(\mathbf O, \mathbf{E} \sigma^2_e)$, 
$\mathbf E$ is a $q \times q$  matrix of similarity based on environmental variables, computed as 
$\mathbf E = \mathbf{XX}^\intercal$, 
$\mathbf X$ is a $q \times c$ matrix of environmental measurements,
$ge \sim N_q(\mathbf O, \mathbf{ZGZ^\intercal \circ E} \sigma^2_{ge})$,
$\epsilon$ is a $q$-vector of residual values, $\epsilon \sim N_q(\mathbf O, \mathbf{I} \sigma^2_\epsilon)$.

These models were fitted in the training set using a Bayesian approach as implemented in the $\sf BGLR$ package.

We used different cross-validation schemes:

* _Random within environment scenario_. We assigned 20\% of the *lines* to the test set randomly and analyzed each sex/temperature 
combination separately using the standard GBLUP model. This procedure was repeated 20 times.
* _Random lines scenario_. We assigned 20\% of the *lines* (i.e., for the 2 sexes and the 3 temperatures) to the test set randomly, 
and analyzed the sex/temperature combinations jointly with the models above. The peculiarity of this scenario is that the lines in 
the test set are not represented at all in the training set. This procedure was repeated 20 times.
* _Random observations scenario_. We assigned 20\% of the *observations* (i.e., combination of line, sex and temperature) to 
the test set randomly, and analyzed the sex/temperature combinations jointly with the models above. The peculiarity of this 
scenario is that all the lines, sexes and temperatures are represented in the training set. This procedure was repeated 20 times.

Prediction accuracy was evaluated as the correlation coefficient between observed and predicted phenotypes in the test set.

## Results

### Random within environment scenario

This is the baseline scenario, where we acknowldege that the genetic architecture of life span is different across environments and we
analyze them separately using G-BLUP.

```{r random within env, fig.height=10, fig.width=13}
sex <- c(1, 0)
temp <- c(18,25,28)
model <- "G"


i <- 0

n_col <- 6
n_row <- n_reps * length(sex) * length(temp) * length(model)
res <- as.data.frame(matrix(NA, ncol=n_col, nrow=n_row))
colnames(res) <- c("rep", "sex", "temp", "model", "r", "env")

for(s in sex){
  for(t in temp){
    for(repp in 1:n_reps){
      dat <- readRDS(paste0("output/G_fit/", prefix, "_random_within_env_", s, "_", t, "_G_fit_", repp, ".rds"))
      i <- i + 1
      
      res[i, 1] <- repp
      res[i, 2] <- s
      res[i, 3] <- t
      res[i, 4] <- model
      res[i, 5] <- cor(dat$y_test, dat$yhat_test)
      res[i, 6] <- paste0(s, "_", t)
    }
  }
}

res <- transform(res, env=as.factor(env))

p <- ggplot(res, aes(x = env, y = r)) +
  geom_boxplot(color = "black", outlier.size = 1, width = 0.85) +
  stat_summary(fun=mean, geom="point", shape=23,
               position = position_dodge2(width = 0.87,   
                                          preserve = "single")) +
  labs(x = "Environment", y = expression(italic(r)), title="") +
  theme_cowplot(font_size = 18)

print(p)

res %>% group_by(env) %>% summarise(mean_r=mean(r), se_r=(sd(r)/n_reps)) %>% as.data.frame()

```

The results show that prediction accuracy varies across environments and is generally low.

### Random lines scenario

In this scenario, we combine the data across environments and fit the different models.

```{r random lines, fig.height=10, fig.width=13}
schemes <- "random_lines"
model <- c("G", "E", "GandE", "GxE")


i <- 0

n_col <- 4
n_row <- n_reps * length(schemes) * length(model)
res <- as.data.frame(matrix(NA, ncol=n_col, nrow=n_row))
colnames(res) <- c("rep", "scheme", "model", "r")

for(sche in schemes){
  for(met in model){
    for(repp in 1:n_reps){
      dat <- readRDS(paste0("output/", met, "_fit/", prefix, "_", sche, "_", met, "_fit_", repp, ".rds"))
      i <- i + 1
        
      res[i, 1] <- repp
      res[i, 2] <- sche
      res[i, 3] <- met
      res[i, 4] <- cor(dat$y_test, dat$yhat_test)
    }
  }
}

res <- transform(res, scheme=as.factor(scheme),
                 model=as.factor(model))

p <- ggplot(res, aes(x = model, y = r, fill = model)) +
  geom_boxplot(color = "black", outlier.size = 1, width = 0.85) +
  stat_summary(fun=mean, geom="point", shape=23,
               position = position_dodge2(width = 0.87,   
                                          preserve = "single")) +
  scale_fill_manual(values = c("pink", "red", "yellow", "orange", "green", "blue", "lightblue")) +
  labs(x = "Model", y = expression(italic(r)), fill="Method", title="") +
  theme_cowplot(font_size = 18)

print(p)

res %>% group_by(model) %>% summarise(mean_r=mean(r), se_r=(sd(r)/n_reps)) %>% as.data.frame()
```

The results show that G-BLUP performs poorly. This is expected because we are making the implicit assumption that genetic
effects are equal across environments, which is not the case. E-BLUP does very well in this scenario because when combining 
the data across environments, a lot of the variance is explained by the environmental effects, which we have good power to 
estimate. GE-BLUP and GxE-BLUP give very similar accuracy to E-BLUP. This is not surprising since some lines are not in the
training set, so the training-test transfer of information happens mostly at the environment level.

### Random observations scenario

In this scenario, every line and environment is potentially represented in the training set. It is specific combinations of
lines and environments (e.g., line_101 in females at 18C) that are not seen in the training set. This is the scenario 
where we expect an advantage from using models with both genotype and environment information because the training-test 
transfer of information happens at every level

```{r random obs, fig.height=10, fig.width=13}
schemes <- "random_obs"
model <- c("G", "E", "GandE", "GxE")


i <- 0

n_col <- 4
n_row <- n_reps * length(schemes) * length(model)
res <- as.data.frame(matrix(NA, ncol=n_col, nrow=n_row))
colnames(res) <- c("rep", "scheme", "model", "r")

for(sche in schemes){
  for(met in model){
    for(repp in 1:n_reps){
      dat <- readRDS(paste0("output/", met, "_fit/", prefix, "_", sche, "_", met, "_fit_", repp, ".rds"))
      i <- i + 1
        
      res[i, 1] <- repp
      res[i, 2] <- sche
      res[i, 3] <- met
      res[i, 4] <- cor(dat$y_test, dat$yhat_test)
    }
  }
}

res <- transform(res, scheme=as.factor(scheme),
                 model=as.factor(model))

p <- ggplot(res, aes(x = model, y = r, fill = model)) +
  geom_boxplot(color = "black", outlier.size = 1, width = 0.85) +
  stat_summary(fun=mean, geom="point", shape=23,
               position = position_dodge2(width = 0.87,   
                                          preserve = "single")) +
  scale_fill_manual(values = c("pink", "red", "yellow", "orange", "green", "blue", "lightblue")) +
  labs(x = "Model", y = expression(italic(r)), fill="Method", title="") +
  theme_cowplot(font_size = 18)

print(p)

res %>% group_by(model) %>% summarise(mean_r=mean(r), se_r=(sd(r)/n_reps)) %>% as.data.frame()
```

The results show that, as expected, GE-BLUP improves prediction accuracy over E-BLUP. GxE-BLUP achieves the highest
prediction accuracy.


